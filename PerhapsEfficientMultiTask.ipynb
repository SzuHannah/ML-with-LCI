{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzuHannah/ML-with-LCI/blob/main/PerhapsEfficientMultiTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOw1lfmFj_qC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy import linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNBMyyUwyChU"
      },
      "source": [
        "#### Implementation of Liu and Ye (2009)\n",
        "- reference  \n",
        "  - [Liu and Ye(2009) paper](https://arxiv.org/pdf/1205.2631.pdf)\n",
        "  - [Original MatLab code](https://github.com/jiayuzhou/SLEP/blob/master/SLEP/functions/L1Lq/Lq1R/mcLeastR.m)  \n",
        "  - [Python code](https://github.com/jundongl/scikit-feature/blob/master/skfeature/function/sparse_learning_based/ls_l21.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "tP6ZgWuTkPZV"
      },
      "outputs": [],
      "source": [
        "# utility functions\n",
        "def euclidean_projection (V, n_features, n_tasks, z, gamma):\n",
        "  \"\"\"\n",
        "  L2-norm Regularized Euclidean Projection:\n",
        "      min 1/2 ||W-V||_2^2 + rho * ||W||_2\n",
        "  which has the closed form solution:\n",
        "      W = {max(||V||_2 - rho , 0) / ||V||_2} * V\n",
        "  where rho = z/gamma\n",
        "  \"\"\"\n",
        "  W_projection = np.zeros((n_features, n_tasks))\n",
        "  for i in range(n_features):\n",
        "    rho = z/gamma\n",
        "    if LA.norm(V[i, :]) > rho: # i.e. when max(||V||_2 - rho , 0) = ||V||_2 - rho\n",
        "      W_projection[i, :] = (1 - rho/LA.norm(V[i, :])) * V[i, :]\n",
        "    else:\n",
        "      W_projection[i, :] = np.zeros(n_tasks)\n",
        "  return W_projection\n",
        "\n",
        "def calculate_l21_norm(X):\n",
        "  \"\"\"\n",
        "  L21-norm of a matrix X:\n",
        "    \\sum ||X[i,:]||_2\n",
        "  \"\"\"\n",
        "  l21_norm = np.sqrt(np.multiply(X, X).sum(1)).sum()\n",
        "  return l21_norm\n",
        "\n",
        "def init_factor(W_norm, XW, Y, z):\n",
        "  \"\"\"\n",
        "  Iinitialize the starting point of W, according to the Liu and Ye (2009)'s code\n",
        "  \"\"\"\n",
        "  n_samples, n_tasks = XW.shape\n",
        "  # numerator = (WX')*Y - z*(W_norm)\n",
        "  a = np.inner(np.reshape(XW, n_samples*n_tasks), np.reshape(Y, n_samples*n_tasks)) - z * W_norm\n",
        "  # denominator = (Frobenius norm of XW) ** 2\n",
        "  b = LA.norm(XW, 'fro') ** 2\n",
        "  ratio = a/b\n",
        "  return ratio\n",
        "\n",
        "\n",
        "## for testing\n",
        "def construct_label_matrix_pan(label):\n",
        "    \"\"\"\n",
        "    This function converts a 1d numpy array to a 2d array, for each instance, the class label is 1 or -1\n",
        "    \"\"\"\n",
        "    n_samples = label.shape[0]\n",
        "    unique_label = np.unique(label)\n",
        "    n_classes = unique_label.shape[0]\n",
        "    label_matrix = np.zeros((n_samples, n_classes))\n",
        "    for i in range(n_classes):\n",
        "        label_matrix[label == unique_label[i], i] = 1\n",
        "    label_matrix[label_matrix == 0] = -1\n",
        "\n",
        "    return label_matrix.astype(int)\n",
        "\n",
        "def feature_ranking(W):\n",
        "    \"\"\"\n",
        "    This function ranks features according to the feature weights matrix W\n",
        "    \"\"\"\n",
        "    T = (W*W).sum(1)\n",
        "    idx = np.argsort(T, 0)\n",
        "    return idx[::-1]\n",
        "\n",
        "def nonzero_feature(W):\n",
        "    \"\"\"\n",
        "    This function non-zero features of the weights matrix W\n",
        "    \"\"\"\n",
        "    T = (W*W).sum(1)\n",
        "    idx = np.argwhere(T!=0)\n",
        "    return idx.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "H-r8dEKy355X"
      },
      "outputs": [],
      "source": [
        "# main functions\n",
        "def init_startingPoint(X, Y, z):\n",
        "  XtY = X.T @ Y # compute X'Y\n",
        "  W = XtY # initialize a starting point\n",
        "  XW = X @ W  # compute XW = X*W\n",
        "  W_norm = calculate_l21_norm(W) #compute l2,1 norm of W\n",
        "  if W_norm >= 1e-6:\n",
        "    ratio = init_factor(W_norm, XW, Y, z)\n",
        "    W = ratio * W\n",
        "    XW = ratio * XW\n",
        "  return XtY, W, XW, W_norm \n",
        "\n",
        "def l21_proximal(X, Y, z, **kwargs):\n",
        "  \"\"\"\n",
        "  X: input data matrix, shape = (n_samples, n_features)\n",
        "  Y: input response matrix, shape = (n_samples, n_tasks)\n",
        "  z: regularization parameter\n",
        "  kwargs: \n",
        "    verbose: True if user want to print out the objective function value in each iteration, false if not\n",
        "  \"\"\"\n",
        "  # process verbose:\n",
        "  if 'verbose' not in kwargs:\n",
        "    verbose = False\n",
        "  else:\n",
        "    verbose = kwargs['verbose']\n",
        "\n",
        "  #shape of X, Y\n",
        "  n_samples, n_features = X.shape\n",
        "  n_samples, n_tasks = Y.shape\n",
        "\n",
        "  # initialization\n",
        "  XtY, W, XW, W_norm = init_startingPoint(X,Y,z)\n",
        "\n",
        "  #--- Armijo Goldstein line search + accelerated gradient descent (proximal gradient descent) ---\n",
        "  # initialize step size gamma = 1\n",
        "  gamma = 1 #gamma=L in the original code\n",
        "\n",
        "  #Initialize Wp (W in previous step) to W; XWp to XW; tp (t in previous step) to 0; t to 1\n",
        "  XWp = XW\n",
        "  WWp = np.zeros((n_features, n_tasks))\n",
        "  tp = 0 #at initialization, this is t_{-1} in the paper, where t_{i} is the upper bound for each ||W(i)||, i.e. ||W(i)|| <= t_{i}\n",
        "  t = 1 #at initialization, this is t_{0} in the paper\n",
        "\n",
        "  #whether converged or not\n",
        "  no_improvement = False\n",
        "  \n",
        "  max_iter = 1000\n",
        "  gammas = np.zeros(max_iter) #collection of the step size for each iteration\n",
        "  objs = np.zeros(max_iter) #collection of objective functions at each iteration\n",
        "\n",
        "  for iter in range(max_iter):\n",
        "    # step 1: compute search point S based on Wp and W' define alpha to be the decreasing factor for line search\n",
        "    alpha = (tp - 1)/t\n",
        "    S = W + alpha*WWp\n",
        "\n",
        "    # step2: linear search for gamma and compute the new apporximation slution W\n",
        "    XS = XW + alpha*(XW - XWp) \n",
        "    XtXS = X.T @ XS # compute X'*XS\n",
        "    GatS = XtXS - XtY # compute gradient G at point S: G = X'(XS-Y) for later use to find the new approximation solution\n",
        "    ## copy W and XW to Wp and XWp\n",
        "    Wp = W\n",
        "    XWp = XW\n",
        "\n",
        "    while True:\n",
        "      ## let S walk in a step in the antigradient of S to get V; then, do L1/L2-norm regularized euclidean projection for V\n",
        "      ## in the paper, it was new_apporx_W = projection(S - 1/gamma * G), where G is gradient at point S\n",
        "      V = S - (1/gamma) * GatS ### ie. V = \"gradient descent step\" of S \n",
        "      W = euclidean_projection(V, n_features, n_tasks, z, gamma) ### new_approx_W \n",
        "      ## difference between the new approximate solution W and the search point S\n",
        "      V = W - S\n",
        "      ## update XW and calculate XV\n",
        "      XW = X @ W\n",
        "      XV = XW - XS\n",
        "      ## define r_sum = sqared(Frobinius norm of V); l_sum = squared(Frobinius norm of XV)\n",
        "      r_sum = LA.norm(V, 'fro')**2\n",
        "      l_sum = LA.norm(XV, 'fro')**2\n",
        "      ## check if the \"gradient descent step\" makes litte improvement\n",
        "      if r_sum <= 1e-20:\n",
        "        no_improvement = True\n",
        "        break\n",
        "      ## the condition is ||XV||_2^2 <= gamma * ||V||_2^2\n",
        "      if l_sum <= gamma * r_sum:\n",
        "        break\n",
        "      else:\n",
        "        gamma = max(2*gamma, l_sum/r_sum)\n",
        "    gammas[iter] = gamma\n",
        "\n",
        "    # step 3: update tp and t, and check if converged\n",
        "    tp = t\n",
        "    t = (1+math.sqrt(1 + 4*t**2)) / 2\n",
        "\n",
        "    WWp = W - Wp\n",
        "    XWY = XW - Y \n",
        "\n",
        "    ## calculate obj\n",
        "    objs[iter] = LA.norm(XWY, 'fro') ** 2 / 2\n",
        "    objs[iter] += z * calculate_l21_norm(W)\n",
        "\n",
        "    if verbose:\n",
        "      print('obj function at iter {0}: {1}'.format(iter+1, objs[iter]))\n",
        "    \n",
        "    if no_improvement is True:\n",
        "      break\n",
        "\n",
        "    ## check convergence\n",
        "    if iter >= 1 and math.fabs(objs[iter] - objs[iter - 1]) < 1e-3:\n",
        "      break\n",
        "  return W, objs, gammas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEF0RrS9zDZC"
      },
      "source": [
        "#### Test on LCI data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyJdm-spzB18"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "X = pd.read_csv(\"X_data.csv\")\n",
        "Y = pd.read_csv(\"Y_data.csv\", usecols = [\"climate change\",\"fossil depletion\",\"metal depletion\", \"human toxicity\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sjA4nlnzYEg"
      },
      "outputs": [],
      "source": [
        "X['size'],sizetype=pd.factorize(X['size'])\n",
        "X['country'],countrytype = pd.factorize(X['country'])\n",
        "X['powertrain'],powertrain = pd.factorize(X['powertrain'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXUfb0cszUv0",
        "outputId": "64712353-e497-430b-c1d7-6fab16335955"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(77, 3, 80)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numericcols = list(X.select_dtypes('float64').columns)\n",
        "numericcols.append(\"year\")\n",
        "catcols = list(X.select_dtypes('int64').columns)\n",
        "catcols.remove(\"year\")\n",
        "cols = X.columns\n",
        "len(numericcols), len(catcols), len(cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dflimmH8oG2"
      },
      "outputs": [],
      "source": [
        "X # country has 7 levels, size has 9 levels, powertrain has 9 levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE8H9jEFv_RH"
      },
      "outputs": [],
      "source": [
        "# comparison test on LCI data\n",
        "## normalize data as suggested by Liu and Ye (2009)'s paper\n",
        "\n",
        "### normalize data to make life easier\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normX = Normalizer().fit_transform(X) \n",
        "normY = Normalizer().fit_transform(Y)\n",
        "\n",
        "## elastic net from sklearn normalize data by default:\n",
        "## ref: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "vXmmM1K301JS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3f7c03-a8e6-4fda-8849-ff764e6e3889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE:  0.04939960199422524\n",
            "Index(['LHV fuel MJ per kg', 'auxilliary power base demand',\n",
            "       'average passenger mass', 'battery cell production energy',\n",
            "       'battery lifetime kilometers',\n",
            "       'battery onboard charging infrastructure cost', 'cargo mass',\n",
            "       'combustion exhaust treatment cost', 'combustion fixed mass',\n",
            "       'combustion powertrain cost per kW', 'cooling thermal demand',\n",
            "       'electric fixed mass', 'electric powertrain cost per kW',\n",
            "       'energy battery cost per kWh', 'energy battery mass',\n",
            "       'fuel cell cost per kW', 'fuel cell lifetime hours',\n",
            "       'fuel cell power area density', 'fuel mass', 'fuel tank cost per kg',\n",
            "       'glider base mass', 'glider cost intercept', 'heat pump cost',\n",
            "       'heating thermal demand', 'inverter mass', 'kilometers per year',\n",
            "       'lifetime kilometers', 'power battery cost per kW', 'year'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "def main(X, Y, colnames):\n",
        "  n_samples, n_features = X.shape\n",
        "  # split data into 5 folds\n",
        "  ss = KFold(n_splits = 5)\n",
        "  reg = linear_model.LinearRegression()\n",
        "\n",
        "  # add up the mse from each fold\n",
        "  total_mse = 0\n",
        "  for train, test in ss.split(X):\n",
        "    W, objs, gammas = l21_proximal(X[train], Y[train], 0.05, verbose = False)\n",
        "    # obtain features with weight > 0\n",
        "    idx = nonzero_feature(W)\n",
        "    selected_features = X[:, idx]\n",
        "    selected_features_colname = colnames[idx]\n",
        "    # use the features to train a linear model (or can use other model to train)\n",
        "    #reg.fit(selected_features[train], Y[train])\n",
        "    #Y_predict = reg.predict(selected_features[test])\n",
        "    Y_predict = X[test] @ W\n",
        "    # obtain mse from the model\n",
        "    mse = mean_squared_error(Y[test], Y_predict)\n",
        "    total_mse = total_mse + mse\n",
        "  average_mse = float(total_mse)/5\n",
        "  #examine W from the last iter\n",
        "  return selected_features_colname, average_mse \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  critical_feats, average_mse = main(normX, normY, X.columns) \n",
        "  print('MSE: ', average_mse)\n",
        "  print(critical_feats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Benchmark"
      ],
      "metadata": {
        "id": "bP_PdgOunsxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPRXZNdEx9v",
        "outputId": "82476b69-3f14-4739-8350-7a195801e7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proximal gradient average time: 35.527\n"
          ]
        }
      ],
      "source": [
        "# timing comparison\n",
        "from sklearn.linear_model import MultiTaskElasticNet\n",
        "import time\n",
        "from statistics import mean\n",
        "def main(X, Y, colnames):\n",
        "  n_samples, n_features = X.shape\n",
        "  # split data into 5 folds\n",
        "  ss = KFold(n_splits = 5)\n",
        "\n",
        "  # l1,2 with proximal gradient\n",
        "  prox_start = time.time()\n",
        "  for train, test in ss.split(X):\n",
        "    W, objs, gammas = l21_proximal(X[train], Y[train], 0.05, verbose = False) # max_iter parameter within the function makes a difference in time cost\n",
        "    # obtain features with weight > 0\n",
        "    idx = nonzero_feature(W)\n",
        "    selected_features = X[:, idx]\n",
        "    selected_features_colname = colnames[idx]\n",
        "    # use the weights to build a linear model\n",
        "    Y_predict_pr = X[test] @ W\n",
        "  \n",
        "  prox_timeCost = time.time()-prox_start\n",
        "  print(\"proximal gradient average time: %.3f\"% prox_timeCost)\n",
        "if __name__ == '__main__':\n",
        "  main(normX, normY, X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import std\n",
        "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
        "#1 time 5-fold\n",
        "start = time.time()\n",
        "model = MultiTaskElasticNet(alpha = 0.05, l1_ratio = 0.05)\n",
        "cv = RepeatedKFold(n_splits = 5, n_repeats = 1, random_state=1)\n",
        "scores = cross_val_score(model, X, Y, scoring = 'neg_mean_squared_error', cv = cv, n_jobs = -1) #taking really long.. maybe change coordinate descent to proximal descent\n",
        "print('MSE: %.3f(%.3f)'%(mean(scores), std(scores)))\n",
        "model.fit(X,Y)\n",
        "duration = time.time() - start\n",
        "print(\"coordinate gradient took %s seconds\"  % duration) # maybe because coordinate descent can take advante of the sparsity of the weight vector, so it's faster\n",
        "                                                        # similarly saw in this: https://github.com/ngmarchant/prox_elasticnet/blob/master/demo/benchmark.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxYfxtfac09r",
        "outputId": "e91802c9-eab6-4178-d09f-5429f927639e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: -0.002(0.000)\n",
            "coordinate gradient took 20.984787225723267 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix"
      ],
      "metadata": {
        "id": "tYLZUoGSnyc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "DqwHAfOzrCbL"
      },
      "outputs": [],
      "source": [
        "# test using the example in the original repo (classification data) to ensure code works\n",
        "import scipy.io\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpOf8QEesfDC",
        "outputId": "2e9222c8-b493-4e4b-9205-d1c20022b032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.975\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # load data\n",
        "    mat = scipy.io.loadmat('COIL20.mat')\n",
        "    X = mat['X']    # data\n",
        "    X = X.astype(float)\n",
        "    y = mat['Y']    # label\n",
        "    y = y[:, 0]\n",
        "    Y = construct_label_matrix_pan(y)\n",
        "    n_samples, n_features = X.shape    # number of samples and number of features\n",
        "\n",
        "    # split data into 10 folds\n",
        "    ss = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    # perform evaluation on classification task\n",
        "    num_fea = 100    # number of selected features\n",
        "    clf = svm.LinearSVC()    # linear SVM\n",
        "\n",
        "    correct = 0\n",
        "    for train, test in ss.split(X):\n",
        "        # obtain the feature weight matrix\n",
        "        Weight, obj, value_gamma = l21_proximal(X[train], Y[train], 0.1, verbose=False)\n",
        "\n",
        "        # sort the feature scores in an ascending order according to the feature scores\n",
        "        idx = feature_ranking(Weight)\n",
        "\n",
        "        # obtain the dataset on the selected features\n",
        "        selected_features = X[:, idx[0:num_fea]]\n",
        "\n",
        "        # train a classification model with the selected features on the training dataset\n",
        "        clf.fit(selected_features[train], y[train])\n",
        "\n",
        "        # predict the class labels of test data\n",
        "        y_predict = clf.predict(selected_features[test])\n",
        "\n",
        "        # obtain the classification accuracy on the test data\n",
        "        acc = accuracy_score(y[test], y_predict)\n",
        "        correct = correct + acc\n",
        "\n",
        "    # output the average classification accuracy over all 10 folds\n",
        "    print('Accuracy:', float(correct)/10)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PerhapsEfficientMultiTask.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUOatI1JxnsW9C6JX/TvSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}